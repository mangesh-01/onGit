{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5992ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de92c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, number, size, inputs=1):\n",
    "        self.number = number\n",
    "        self.size = size\n",
    "        self.data = np.zeros((inputs, size), dtype=np.float64)\n",
    "\n",
    "class Weight:\n",
    "    def __init__(self, prev, next):\n",
    "        self.value = (np.random.randn(prev.size, next.size).astype(np.float64))\n",
    "\n",
    "class Bias:\n",
    "    def __init__(self, next):\n",
    "        self.value = np.random.randn(1, next.size).astype(np.float64)\n",
    "\n",
    "class Neural:\n",
    "    def __init__(self, activation):\n",
    "        self.activation = activation\n",
    "        self.layers = []\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        if len(self.layers) > 1:\n",
    "            self.weights.append(Weight(self.layers[-2], self.layers[-1]))\n",
    "            self.biases.append(Bias(self.layers[-1]))\n",
    "\n",
    "    def forward(self):\n",
    "        self.z_values = []\n",
    "        self.a_values = []\n",
    "        for i in range(1, len(self.layers)):\n",
    "            prev_data = self.layers[i - 1].data\n",
    "            w = self.weights[i - 1].value\n",
    "            b = self.biases[i - 1].value\n",
    "            z = np.dot(prev_data, w) + b\n",
    "            self.z_values.append(z)\n",
    "            a = self.activation(z)\n",
    "            self.a_values.append(a)\n",
    "            self.layers[i].data = a\n",
    "\n",
    "    @property\n",
    "    def output(self):\n",
    "        return self.layers[-1].data\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def costFunction(predicted, real):\n",
    "    return np.sum((predicted - real) ** 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa96a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- Text Processor ---------------------- #\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"<.*?>\", \"\", text)\n",
    "        text = re.sub(r\"[^a-zA-Z']\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        words = set()\n",
    "        for sentence in texts:\n",
    "            words.update(sentence.split())\n",
    "        self.vocab = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "    def vectorize(self, sentence):\n",
    "        vec = np.zeros(len(self.vocab))\n",
    "        for word in sentence.split():\n",
    "            if word in self.vocab:\n",
    "                vec[self.vocab[word]] += 1\n",
    "        return vec\n",
    "\n",
    "    def process(self, df, text_column):\n",
    "        df[text_column] = df[text_column].apply(self.clean_text)\n",
    "        self.build_vocab(df[text_column])\n",
    "        vectors = np.array([self.vectorize(text) for text in df[text_column]])\n",
    "        return vectors / np.max(vectors)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53d5c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df = df[:2000]\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "processor = TextProcessor()\n",
    "X = processor.process(df, 'review')\n",
    "y = df['sentiment'].values.reshape(-1, 1)\n",
    "\n",
    "model = Neural(sigmoid)\n",
    "model.add_layer(Layer(0, X.shape[1], inputs=X.shape[0]))\n",
    "model.layers[0].data = X\n",
    "model.add_layer(Layer(1, 32, inputs=X.shape[0]))\n",
    "model.add_layer(Layer(2, 16, inputs=X.shape[0]))\n",
    "model.add_layer(Layer(3, 1, inputs=X.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "699bd7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 924.0395\n",
      "Epoch 2, Loss: 922.0602\n",
      "Epoch 3, Loss: 919.9797\n",
      "Epoch 4, Loss: 917.7907\n",
      "Epoch 5, Loss: 915.4852\n",
      "Epoch 6, Loss: 913.0545\n",
      "Epoch 7, Loss: 910.4893\n",
      "Epoch 8, Loss: 907.7792\n",
      "Epoch 9, Loss: 904.9130\n",
      "Epoch 10, Loss: 901.8782\n",
      "Epoch 11, Loss: 898.6613\n",
      "Epoch 12, Loss: 895.2477\n",
      "Epoch 13, Loss: 891.6210\n",
      "Epoch 14, Loss: 887.7635\n",
      "Epoch 15, Loss: 883.6559\n",
      "Epoch 16, Loss: 879.2768\n",
      "Epoch 17, Loss: 874.6032\n",
      "Epoch 18, Loss: 869.6097\n",
      "Epoch 19, Loss: 864.2688\n",
      "Epoch 20, Loss: 858.5509\n",
      "Epoch 21, Loss: 852.4238\n",
      "Epoch 22, Loss: 845.8532\n",
      "Epoch 23, Loss: 838.8025\n",
      "Epoch 24, Loss: 831.2336\n",
      "Epoch 25, Loss: 823.1068\n",
      "Epoch 26, Loss: 814.3819\n",
      "Epoch 27, Loss: 805.0197\n",
      "Epoch 28, Loss: 794.9829\n",
      "Epoch 29, Loss: 784.2391\n",
      "Epoch 30, Loss: 772.7632\n",
      "Epoch 31, Loss: 760.5412\n",
      "Epoch 32, Loss: 747.5748\n",
      "Epoch 33, Loss: 733.8860\n",
      "Epoch 34, Loss: 719.5226\n",
      "Epoch 35, Loss: 704.5636\n",
      "Epoch 36, Loss: 689.1227\n",
      "Epoch 37, Loss: 673.3506\n",
      "Epoch 38, Loss: 657.4332\n",
      "Epoch 39, Loss: 641.5863\n",
      "Epoch 40, Loss: 626.0449\n",
      "Epoch 41, Loss: 611.0482\n",
      "Epoch 42, Loss: 596.8228\n",
      "Epoch 43, Loss: 583.5647\n",
      "Epoch 44, Loss: 571.4247\n",
      "Epoch 45, Loss: 560.4998\n",
      "Epoch 46, Loss: 550.8299\n",
      "Epoch 47, Loss: 542.4027\n",
      "Epoch 48, Loss: 535.1615\n",
      "Epoch 49, Loss: 529.0180\n",
      "Epoch 50, Loss: 523.8635\n",
      "Epoch 51, Loss: 519.5803\n",
      "Epoch 52, Loss: 516.0504\n",
      "Epoch 53, Loss: 513.1613\n",
      "Epoch 54, Loss: 510.8104\n",
      "Epoch 55, Loss: 508.9064\n",
      "Epoch 56, Loss: 507.3703\n",
      "Epoch 57, Loss: 506.1351\n",
      "Epoch 58, Loss: 505.1443\n",
      "Epoch 59, Loss: 504.3511\n",
      "Epoch 60, Loss: 503.7171\n",
      "Epoch 61, Loss: 503.2110\n",
      "Epoch 62, Loss: 502.8073\n",
      "Epoch 63, Loss: 502.4856\n",
      "Epoch 64, Loss: 502.2292\n",
      "Epoch 65, Loss: 502.0249\n",
      "Epoch 66, Loss: 501.8621\n",
      "Epoch 67, Loss: 501.7323\n",
      "Epoch 68, Loss: 501.6288\n",
      "Epoch 69, Loss: 501.5461\n",
      "Epoch 70, Loss: 501.4800\n",
      "Epoch 71, Loss: 501.4271\n",
      "Epoch 72, Loss: 501.3846\n",
      "Epoch 73, Loss: 501.3505\n",
      "Epoch 74, Loss: 501.3229\n",
      "Epoch 75, Loss: 501.3005\n",
      "Epoch 76, Loss: 501.2822\n",
      "Epoch 77, Loss: 501.2673\n",
      "Epoch 78, Loss: 501.2550\n",
      "Epoch 79, Loss: 501.2447\n",
      "Epoch 80, Loss: 501.2360\n",
      "Epoch 81, Loss: 501.2287\n",
      "Epoch 82, Loss: 501.2224\n",
      "Epoch 83, Loss: 501.2169\n",
      "Epoch 84, Loss: 501.2121\n",
      "Epoch 85, Loss: 501.2078\n",
      "Epoch 86, Loss: 501.2039\n",
      "Epoch 87, Loss: 501.2003\n",
      "Epoch 88, Loss: 501.1970\n",
      "Epoch 89, Loss: 501.1938\n",
      "Epoch 90, Loss: 501.1909\n",
      "Epoch 91, Loss: 501.1880\n",
      "Epoch 92, Loss: 501.1853\n",
      "Epoch 93, Loss: 501.1827\n",
      "Epoch 94, Loss: 501.1801\n",
      "Epoch 95, Loss: 501.1775\n",
      "Epoch 96, Loss: 501.1751\n",
      "Epoch 97, Loss: 501.1726\n",
      "Epoch 98, Loss: 501.1702\n",
      "Epoch 99, Loss: 501.1678\n",
      "Epoch 100, Loss: 501.1654\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "epochs = 100\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.forward()\n",
    "    predicted = model.output\n",
    "    loss = costFunction(predicted, y)\n",
    "    dL = 2 * (predicted - y) / y.shape[0]\n",
    "\n",
    "    for i in reversed(range(len(model.weights))):\n",
    "        z = model.z_values[i]\n",
    "        a_prev = model.a_values[i - 1] if i > 0 else model.layers[0].data\n",
    "        dz = dL * model.a_values[i] * (1 - model.a_values[i])\n",
    "\n",
    "        dw = np.dot(a_prev.T, dz)\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "\n",
    "        model.weights[i].value -= lr * dw\n",
    "        model.biases[i].value -= lr * db\n",
    "\n",
    "        dL = np.dot(dz, model.weights[i].value.T)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bba622bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"It was great product.\"\n",
    "cleaned = processor.clean_text(text)\n",
    "vec = processor.vectorize(cleaned).reshape(1, -1)\n",
    "vec = vec / np.max(vec)\n",
    "model.layers[0].data = vec\n",
    "model.forward()\n",
    "output = model.output[0][0]\n",
    "print(\"Sentiment:\", \"Positive\" if output > 0.5 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0515e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
