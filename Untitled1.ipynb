{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUWXjQjphtq3MeooB+c2Xm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXlGzLgGTACt","outputId":"163cbcde-91eb-460c-901c-ea50f72b119f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30 - Loss: 2.95987\n","Epoch 2/30 - Loss: 2.13927\n","Epoch 3/30 - Loss: 2.07993\n","Epoch 4/30 - Loss: 1.96070\n","Epoch 5/30 - Loss: 1.91731\n","Epoch 6/30 - Loss: 1.87971\n","Epoch 7/30 - Loss: 1.87120\n","Epoch 8/30 - Loss: 1.77505\n","Epoch 9/30 - Loss: 1.74330\n","Epoch 10/30 - Loss: 1.74159\n","Epoch 11/30 - Loss: 1.76035\n","Epoch 12/30 - Loss: 1.70250\n","Epoch 13/30 - Loss: 1.68068\n","Epoch 14/30 - Loss: 1.58847\n","Epoch 15/30 - Loss: 1.60249\n","Epoch 16/30 - Loss: 1.54508\n","Epoch 17/30 - Loss: 1.49204\n","Epoch 18/30 - Loss: 1.47767\n","Epoch 19/30 - Loss: 1.45691\n","Epoch 20/30 - Loss: 1.41359\n"]}],"source":["\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from IPython.display import display\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","def relu(x):\n","    return np.maximum(0, x)\n","\n","def relu_prime(x):\n","    return (x > 0).astype(float)\n","\n","def softmax(x):\n","    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    return exps / np.sum(exps, axis=1, keepdims=True)\n","\n","\n","def cross_entropy(y_pred, y_true):\n","    eps = 1e-12\n","    y_pred = np.clip(y_pred, eps, 1 - eps)\n","    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n","\n","transform = transforms.ToTensor()\n","cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","\n","sample_size = 1000\n","images = []\n","labels = []\n","for i in range(sample_size):\n","    img, label = cifar10[i]\n","    images.append(img.numpy().reshape(-1))\n","    labels.append(label)\n","\n","X = np.stack(images)\n","X = X / 1.0\n","\n","Y = np.zeros((sample_size, 10), dtype=np.float32)\n","for i, label in enumerate(labels):\n","    Y[i, label] = 1\n","\n","class Layer:\n","    def __init__(self, in_size, out_size):  # Fixed: __init__ instead of _init_\n","        self.W = np.random.randn(in_size, out_size) * np.sqrt(2. / in_size)\n","        self.B = np.zeros((1, out_size))\n","        self.mW = np.zeros_like(self.W)\n","        self.vW = np.zeros_like(self.W)\n","        self.mB = np.zeros_like(self.B)\n","        self.vB = np.zeros_like(self.B)\n","\n","class Container:\n","    layers = []\n","\n","    @staticmethod\n","    def add(layer):\n","        Container.layers.append(layer)\n","\n","    @staticmethod\n","    def reset():\n","        Container.layers.clear()\n","\n","    @staticmethod\n","    def get(i):\n","        return Container.layers[i]\n","\n","class NeuralNetwork:\n","    def __init__(self):\n","        self.layers = 0\n","        self.costs = []\n","\n","    def add_layer(self, in_size, out_size):\n","        Container.add(Layer(in_size, out_size))\n","        self.layers += 1\n","\n","    def forward(self, X):\n","        self.A = [X]\n","        self.Z = []\n","        for l in range(self.layers - 1):\n","            Z = np.dot(self.A[-1], Container.get(l).W) + Container.get(l).B\n","            A = relu(Z)\n","            self.Z.append(Z)\n","            self.A.append(A)\n","        Z = np.dot(self.A[-1], Container.get(self.layers - 1).W) + Container.get(self.layers - 1).B\n","        A = softmax(Z)\n","        self.Z.append(Z)\n","        self.A.append(A)\n","        return self.A[-1]\n","\n","    def backward(self, Y, lr, t, beta1=0.9, beta2=0.999, eps=1e-8):\n","        m = Y.shape[0]\n","        dZ = self.A[-1] - Y\n","        for l in reversed(range(self.layers)):\n","            dW = np.dot(self.A[l].T, dZ) / m\n","            dB = np.sum(dZ, axis=0, keepdims=True) / m\n","            if l > 0:\n","                dA = np.dot(dZ, Container.get(l).W.T)\n","                dZ = dA * relu_prime(self.Z[l-1])\n","\n","            layer = Container.get(l)\n","            layer.mW = beta1 * layer.mW + (1 - beta1) * dW\n","            layer.vW = beta2 * layer.vW + (1 - beta2) * (dW ** 2)\n","            mW_corr = layer.mW / (1 - beta1 ** t)\n","            vW_corr = layer.vW / (1 - beta2 ** t)\n","\n","            layer.mB = beta1 * layer.mB + (1 - beta1) * dB\n","            layer.vB = beta2 * layer.vB + (1 - beta2) * (dB ** 2)\n","            mB_corr = layer.mB / (1 - beta1 ** t)\n","            vB_corr = layer.vB / (1 - beta2 ** t)\n","\n","            layer.W -= lr * mW_corr / (np.sqrt(vW_corr) + eps)\n","            layer.B -= lr * mB_corr / (np.sqrt(vB_corr) + eps)\n","\n","    def train(self, X, Y, epochs=30, lr=0.001, batch_size=64):\n","        m = X.shape[0]\n","        t = 0\n","        for e in range(epochs):\n","            indices = np.arange(m)\n","            np.random.shuffle(indices)\n","            X_shuffled = X[indices]\n","            Y_shuffled = Y[indices]\n","\n","            epoch_cost = 0\n","            for i in range(0, m, batch_size):\n","                t += 1\n","                X_batch = X_shuffled[i:i+batch_size]\n","                Y_batch = Y_shuffled[i:i+batch_size]\n","                Yhat = self.forward(X_batch)\n","                cost = cross_entropy(Yhat, Y_batch)\n","                epoch_cost += cost * X_batch.shape[0]\n","                self.backward(Y_batch, lr, t)\n","\n","            avg_cost = epoch_cost / m\n","            self.costs.append(avg_cost)\n","            print(f\"Epoch {e+1}/{epochs} - Loss: {avg_cost:.5f}\")\n","\n","        plt.plot(self.costs)\n","        plt.title(\"Training Loss\")\n","        plt.xlabel(\"Epoch\")\n","        plt.ylabel(\"Loss\")\n","        plt.grid(True)\n","        plt.show()\n","\n","Container.reset()\n","mlp = NeuralNetwork()\n","mlp.add_layer(3072, 512)\n","mlp.add_layer(512, 128)\n","mlp.add_layer(128, 10)\n","mlp.train(X, Y, epochs=30, lr=0.001, batch_size=64)\n","\n","test_set = torchvision.datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n","\n","test_size = 1000\n","test_images = []\n","test_labels = []\n","for i in range(test_size):\n","    img, label = test_set[i]\n","    test_images.append(img.numpy().reshape(-1))\n","    test_labels.append(label)\n","\n","X_test = np.stack(test_images)\n","X_test = X_test / 1.0\n","Y_test_labels = np.array(test_labels)\n","\n","preds = mlp.forward(X_test)\n","pred_labels = np.argmax(preds, axis=1)\n","\n","classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","           'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","num_show = int(input(\"ENTER NUMBER OF TEST IMAGES TO DISPLAY: \"))\n","\n","for i in range(num_show):\n","    img = X_test[i].reshape(3, 32, 32).transpose(1,2,0)\n","    pred = classes[pred_labels[i]]\n","    actual = classes[Y_test_labels[i]]\n","    print(f\"Predicted: {pred}, Actual: {actual}\")\n","    display(Image.fromarray((img * 255).astype(np.uint8)))\n","\n","accuracy = np.mean(pred_labels == Y_test_labels)\n","test_cost = cross_entropy(preds, np.eye(10)[Y_test_labels])\n","\n","print(f\"\\nTest Loss on {test_size} samples: {test_cost:.5f}\")\n","print(f\"Test Accuracy on {test_size} samples: {accuracy * 100:.2f}%\")"]}]}